---
author: "Matthew Arsenault and Kelvin Bacelli"
title: "Your descriptive title"
date: "2025-10-08"
output:
  pdf_document:
    number_sections: true
---
```{r echo=FALSE, include=FALSE, message=FALSE}
# all library load statements here
library(caret)
library(knitr)
r.version <- "4.3.2"
seed.val <- 123456
```

Note: Brief intro and your main question go in this space. You may edit the headings and outline as fits your analysis. 

# Data Description and Processing

The dataset we used was compiled by the used auto industry and contains information related to the sale of pre-owned vehicles. The data provide insights into various attributes of used cars that may influence their selling price.

This dataset contains several types of observations describing vehicle characteristics and ownership details for a range of car listings.

Outcomes:\
selling_price: Selling price of the vehicle (in U.S. dollars). \\
Vehicle Attributes:\
km_driven: Total distance the car has been driven (in kilometers).\
mileage: Fuel efficiency of the vehicle (in miles per gallon).\
engine: Engine capacity (in cubic centimeters).\
max_power: Maximum power output of the vehicle (in brake horsepower).\
seats: Number of seats available in the car. \
transmission: Type of transmission, either manual or automatic.\
Ownership:\
seller_type: Indicates whether the seller is an individual or a dealer.\
owner: Level of ownership - First Owner, Second Owner, Third Owner, or Fourth & Above Owner. \

The dataset was examined for missing values. Of the original 3,425 rows, 111 contained one or more missing entries. These 111 rows were removed, leaving a total of 3,314 complete observations for analysis.

```{r echo=FALSE, include=FALSE}
data.df <- read.csv("dataset3.csv")
data.df <- na.omit(data.df)
data.df$transmission <- factor(data.df$transmission)
data.df$owner <- factor(data.df$owner)
data.df$seller_type <- factor(data.df$seller_type)
```



# Data Exploration

The dataset contains observations on used cars with varying numbers of previous owners. Since vehicle ownership history is likely to influence selling price, we compare the number of vehicles across ownership levels. The distribution shows that most cars are first-owner vehicles(~66%), with progressively fewer cars in each subsequent ownership category.

\newpage

```{r echo=FALSE}
label = c("First Owner","Second Owner", "Third Owner","Fourth & Above Owner")
count = c("2204","801","233","76")
owner.tbl <- cbind(label,count)
kable(owner.tbl, caption = "Owner Comparison")
```

We also explored the distribution of our outcome variable, selling price, to understand the overall spread of car values in the dataset. The distribution is heavily right-skewed, indicating that while most vehicles sell for relatively low prices, a small number of cars have very high selling prices.

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Selling Price (US dollars)."}
hist(data.df$selling_price, freq = FALSE, main = "", xlab = "Value",ylab = "Density")

lines(density(data.df$selling_price),col = "red",lwd=2)
```


```{r echo=FALSE}
# table of descriptive stats for the overall life expectancy
Statistics <- c("Min", "Mean","Median","StdDev","Max")
mn<-min(data.df$selling_price)
mx<-max(data.df$selling_price)
avg<-round(mean(data.df$selling_price), 2)
md<-median(data.df$selling_price)
std<-round(sd(data.df$selling_price), 2)
Value <- c(mn,avg,md,std,mx)
st.tbl <- cbind(Statistics, Value)
kable(st.tbl, caption="Overall Selling Price Descriptive Stats")
```
\newpage
Due to the fact that the original selling price variable was highly right-skewed, we applied the natural logarithm to the graph in order to make the distribution more symmetric. This helps stabilize the variance and makes the data more suitable for statistical modeling techniques (which assume approximately normal residuals). Because the natural log scale is exponential, each one-unit increase in log(selling price) corresponds to roughly a 2.7x increase in the actual selling price, meaning values from 10 to 11 represent about a 2.7 times increase, and from 11 to 12 another 2.7 times increase.

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Selling Price (US dollars)."}

hist(log(data.df$selling_price), freq = FALSE, main = "", xlab = "Log(Selling Price)",ylab = "Density")

lines(density(log(data.df$selling_price)),col = "red",lwd=2)
```

We created a boxplot of the log-transformed selling price across the four ownership categories to explore how the number of previous owners affects vehicle value. As shown in the plot, as the ownership level increases, the mean selling price decreases, indicating that vehicles with more previous owners tend to have lower market values.

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Selling Price by Ownership Level."}
data.df$owner <- trimws(data.df$owner)  # remove any stray spaces

data.df$owner <- factor(data.df$owner,levels = c("First Owner", "Second Owner", "Third Owner", "Fourth & Above Owner"))

par(mar = c(8,4,4,2))
boxplot(log(selling_price)~owner,data = data.df,main="",ylab = "Log(Selling Price)", xlab = "Level of Ownership", las=1,cex.axis=0.8)

```


```{r echo=FALSE, include=FALSE}
first.df  <- subset(data.df, owner == "First Owner")
second.df <- subset(data.df, owner == "Second Owner")
third.df  <- subset(data.df, owner == "Third Owner")
fourth.df <- subset(data.df, owner == "Fourth & Above Owner")

```

```{r echo=FALSE}
# table of descriptive stats for the overall life expectancy
statistics <- c("Min", "Mean","Median","StdDev","Max")
#developed
data1<-first.df$selling_price
mn1<-min(data1)
mx1<-max(data1)
avg1<-round(mean(data1), 2)
md1<-median(data1)
std1<-round(sd(data1), 2)
first <- c(mn1,avg1,md1,std1,mx1)
#developing
data2<-second.df$selling_price
mn2<-min(data2)
mx2<-max(data2)
avg2<-round(mean(data2), 2)
md2<-median(data2)
std2<-round(sd(data2), 2)
second <- c(mn2,avg2,md2,std2,mx2)

data3<-third.df$selling_price
mn3<-min(data3)
mx3<-max(data3)
avg3<-round(mean(data3), 2)
md3<-median(data3)
std3<-round(sd(data3), 2)
third <- c(mn3,avg3,md3,std3,mx3)

data4<-fourth.df$selling_price
mn4<-min(data4)
mx4<-max(data4)
avg4<-round(mean(data4), 2)
md4<-median(data4)
std4<-round(sd(data4), 2)
fourth <- c(mn4,avg4,md4,std4,mx4)

tble.df<- data.frame(statistics, first, second, third, fourth)
kable(tble.df, caption="Selling Price per Status Descriptive Stats")

```

A one-way ANOVA was conducted to test whether mean log(selling_price) differed across ownership levels. The results were highly significant, meaning at least one group differed from the others. A Tukey post-hoc test revealed that all pairwise comparisons between ownership levels were statistically significant (p<0.001), further reinforcing the trend that car value decreases as the number of previous owners increases.

```{r echo=FALSE, include=FALSE} 
anova_result <- aov(log(selling_price)~owner,data = data.df)
TukeyHSD(anova_result)
```
\newpage

# Modeling 

The variable selling_price represents the outcome of interest - the market value (US Dollars) of a used vehicle. Predictor variables such as year and km_driven reflect the vehicle's age and usage, while fuel, seller_type, and transmission describe its technical and commercial characteristics. The variable owner captures ownership history, which may indicate how well the car was maintained. Together, these predictors represent both physical wear and market perception factors that could affect resale value. Exploring their relationship with selling price can reveal which characteristics most strongly determine how much a vehicle sells for.

## Hypothesis

We believe that the selling price of used cars will be positively correlated with the lower amount of kilometers driven and fewer amount of previous owners. Additionally, we expect a positive correlation with the higher mileage efficiency, a greater number of seats, a higher maximum power, and a larger engine size.k dd


## Model 1:

We fit a linear model selling_price as our outcome variable and the predictor variables km_driven, seller_type, transmission, owner, mileage, engine, max_power, and seats to mainly get a better idea of what values are more significant than others. The results are as followed:

\newpage

```{r echo=FALSE}
fit.all <- lm(selling_price ~ km_driven+seller_type+transmission+owner+mileage+engine+max_power+seats, data = data.df)
vals <- coef(summary(fit.all))
colnames(vals)[4] <- "p value"
vals <- round(vals, 4)
kable(vals)
```

The adjusted R-squared value for this model was 0.6747, meaning that around 67.5% of the variation of selling price can be explained by the predictors included in the model. This suggests that this model has a reasonably good fit.

The following is a residual histogram and a Q-Q plot to show how closely your residuals follow a normal distribution. The histogram displays a strong peak at the center with evident skewness and a few big outliers. The same is also reflected in the Q-Q plot, indicating that the model is not perfectly normal.

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Distribution of Residuals: Model 1."}
par(mfrow=c(1,2))
hist(fit.all$residuals, freq = FALSE, main = "", xlab = "Residuals", xlim = range(fit.all$residuals), ylim = c(0, max(density(fit.all$residuals)$y) * 1.1))
lines(density(fit.all$residuals), lwd = 2)


qqnorm(y=fit.all$residuals, main = "")
qqline(y=fit.all$residuals, datax = FALSE)
```

After examining our data, we found that cars with higher fuel efficiency, greater engine power, and more seats tended to sell for higher prices, while cars with more kilometers driven, manual transmissions, individual sellers, and multiple previous owners were associated with lower prices. Notably, second and third-owner vehicles were cheaper than first-owner cars, whereas having four or more previous owners did not significantly affect price (p = 0.4383). Similarly, although the number of seats generally increased in value, its effect was less pronounced in some cases. 
\newpage
As we found earlier, applying the log() function onto our dependent variable selling_price because of its skew helped the histograms distribution. We tried applying this same function to our linear regression model to the dependent variable selling_price, and a dependent variable that also has a skew, km_driven. The results are shown below.

```{r echo=FALSE}
fit.log <- lm(log(selling_price) ~ log(km_driven) +seller_type+transmission+owner+mileage+engine+max_power+seats, data = data.df)
vals <- coef(summary(fit.log))
colnames(vals)[4] <- "p value"
vals <- round(vals, 4)
kable(vals)
```

While this table just shows the same numbers in a different format, the adjusted R-squared for this model came out to be .7501, again meaning that around 75% of the variation of selling price can be explained by the predictors in the model. The previous model that has the same predictors without the log function is shown to only predict around 67.5% of the data, so getting rid of the skew helps the model more accurately explain the variation. 

Below is another residual histogram and Q-Q plot for the new model. Compared to the previous model, the histogram now follows a distribution that is much closer to normal, and the Q-Q plot shows noticeably reduced right-skewness. This improvement makes sense with our expectations, as applying a logarithmic transformation helps correct right-skewed data and stabilize variance.

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Distribution of Residuals: Model 1."}
par(mfrow=c(1,2))
hist(fit.log$residuals, freq = FALSE, main = "", xlab = "Residuals", xlim = range(fit.log$residuals), ylim = c(0, max(density(fit.log$residuals)$y) * 1.1))
lines(density(fit.log$residuals), lwd = 2)

qqnorm(y=fit.log$residuals, main = "")
qqline(y=fit.log$residuals, datax = FALSE)
```


## Model 2:

When a dataset contains many variables, it is common for some of them to be related to each other. In our case, with car data, several variables appear to have semi-linear relationships. For instance, as the number of previous owners increases, it makes sense that the car would have a higher number of kilometers driven. To explore this relationship further, we introduced an interaction term between owner and km_driven (owner * log(km_driven)), which causes the data to be easier to interpret, and allows the effect of kilometers driven on the selling price to vary depending on ownership status. We then incorporated this interaction into a new linear model to examine whether accounting for this relationship improves model performance.

```{r echo=FALSE}
fit.new <- lm(log(selling_price) ~ log(km_driven) * owner + seller_type + transmission + engine + max_power + mileage + seats, data = data.df)
new.vals <- coef(summary(fit.new))
colnames(new.vals)[4] <- "p value"
new.vals <- round(new.vals, 4)
kable(new.vals)
```
\newpage
The adjusted R-squared value for this model was 0.751 which not by much, but does make an improvement to the amount of variation explained compared to the previous model's adjusted R-squared of 0.7501. In this new model, we also see that the negative and significant coefficients for log(km_driven):ownerSecond Owner and log(km_driven):ownerFourth & Above Owner suggest that for cars with more previous owners, each additional kilometer driven is associated with a larger decrease in selling price compared to first-owner cars. This makes sense as buyers likely perceive higher mileage as a greater depreciation factor for cars that don't have the title of being owned only once before.

The following is a residual histogram and a Q-Q plot to show how closely our residuals follow a normal distribution for our new model. With no variables being changed besides owners and km_driven, the histogram and Q-Q plot show nearly the same as the previous model.

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Distribution of Residuals: Model 1."}
par(mfrow=c(1,2))
hist(fit.new$residuals, freq = FALSE, main = "", xlab = "Residuals", xlim = range(fit.new$residuals), ylim = c(0, max(density(fit.new$residuals)$y) * 1.1))
lines(density(fit.new$residuals), lwd = 2)


qqnorm(y=fit.new$residuals, main = "")
qqline(y=fit.new$residuals, datax = FALSE)
```

Overall, Model 2 represents a slight improvement over Model 1, not only by explaining a bit more of the variability in selling price, but also by making the relationships in the data more interpretable.

\newpage

# Predicting with Models 1 and 2

Now that we have two distinct models, we can work with both of them to see which one is a better predictor of selling price given data. We can use a 10-fold cross validation to do this, seperating 80% of the data to use for training, and the other 20% for testing on each fold.

```{r echo=FALSE}
train.control <- trainControl(method = "cv", number = 10, p = 0.80) 
RNGversion(r.version)
set.seed(seed.val)

all.train <- train(log(selling_price) ~ log(km_driven)+seller_type+transmission+owner+mileage+engine+max_power+seats, data = data.df, method = "lm", trControl = train.control)
new.train <- train(log(selling_price) ~ log(km_driven)*owner +seller_type+transmission+mileage+engine+max_power+seats, data = data.df, method = "lm", trControl = train.control)
```


```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="RMSE Over 10 Folds."}
num.folds <- 10
rmse.all <- all.train$resample$RMSE
rmse.new <- new.train$resample$RMSE
plot(rmse.all, xlab = "Fold", xaxp = c(1, num.folds, num.folds-1), ylab = "RMSE", ylim=c(min(rmse.new), max(rmse.all)), col = "blue")
lines(rmse.all, col = "blue")
points(rmse.new, col="red")
lines(rmse.new, col = "red",lty = 2)
legend("bottomright", legend=c("Model 1", "Model 2"), col=c("blue", "red"), lty=1:2, cex=0.5)
```

We can now calculate the average root mean squared error (RMSE) for both of our models' predictions of the 10 folds. The results are below.

```{r echo=FALSE}
label <- c("Model 1", "Model 2")
avg.rmse.all<-round(mean(rmse.all),2)
avg.rmse.new<-round(mean(rmse.new),2)
rmse <- c(avg.rmse.all, avg.rmse.new)
imb.tbl <- cbind(label, rmse)
kable(imb.tbl, caption="Mean RMSE for Models 1 and 2.")
```

\newpage

# Summary and Conclusions

Both model 1 and model 2 fit the data well, and shows a positive, significant correlation with selling price for the variables that measured mileage, max power, and seats while showing a significant, negative correlation with the variables measuring kilometers driven, seller type (individual), transmission (manual), owner, and engine.

Model 2 demonstrated a slightly better fit, making it the stronger predictor of selling price. The inclusion of the interaction term allowed model 2 to capture a bit more variation in the data, marginally improving its predictive accuracy.

It is important to note that this dataset represents only a subset of the used car market, and that many other factors- such as brand reputation, regional demand, economic conditions, and maintenance history- can also affect selling price.

With these considerations in mind, we can conclude that, based on our models, higher mileage efficiency, greater engine power, and more seating capacity are associated with higher selling prices, while cars with more kilometers driven, manual transmissions, or individual sellers tend to have lower selling prices in this dataset.

